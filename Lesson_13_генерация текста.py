# -*- coding: utf-8 -*-
"""HW_13_Ultra_Генерация текста.ipynb""

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uk632Y2S78Brqkt8-35DhBczCkFHbOBJ

Добейтесь максимально низкого Loss’а в обучении модели, экспериментируя с архитектурой и параметрами сети(размеры пространства эмбеддинга, слоев сетки, разные оптимайзеры и т.п)

## Импорт библиотек и данных
"""

from google.colab import files # модуль для загрузки файлов в colab
import numpy as np #библиотека для работы с массивами данных
import os
import matplotlib.pyplot as plt

from tensorflow.keras.models import Model, load_model # из кераса подгружаем абстрактный класс базовой модели, метод загрузки предобученной модели
from tensorflow.keras.layers import Dense, Embedding, LSTM, Input # из кераса загружаем необходимые слои для нейросети
from tensorflow.keras.optimizers import RMSprop, Adam, Nadam, Adadelta, Adamax # из кераса загружаем выбранный оптимизатор

from tensorflow.keras.preprocessing.sequence import pad_sequences # загружаем метод ограничения последовательности заданной длиной
from tensorflow.keras.preprocessing.text import Tokenizer # загружаем токенизатор кераса для обработки текста

from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from IPython.display import clear_output

from tensorflow.keras import utils # загружаем утилиты кераса для one hot кодировки
from tensorflow.keras.utils import plot_model # удобный график для визуализации архитектуры модели

import yaml # импортируем модуль для удобной работы с файлами

import logging
logging.disable(logging.WARNING)

from google.colab import drive
drive.mount('/content/drive')

os.chdir('/content/drive/My Drive/Colab Notebooks/Lesson 13')

#os.chdir('/content/drive/My Drive/Занятие_13_Генерация текста/data')
#decoderForInput, decoderForOutput, encoderForInput = np.load('decoderForInput.npy'), np.load('decoderForOutput.npy'), np.load('encoderForInput.npy')
#print(len(decoderForInput), len(decoderForOutput), len(encoderForInput))

"""## Парсинг данных"""

f = open('answer_database_3.txt', mode='r', encoding='cp1251')
f.read(50)

conversation1, ac1 = [], []

a, b = 0, 0
for line in f:

  m = line.split('\\')
  #if m[0][0] == '?': m[0][0:2].replace('?', m[0][1])
  l = [str(m[0]), str(m[1])]
  if 8<len(m[0])<15 and 8<len(m[1])<15 : 
    conversation1.append(l)
    if m[0]: a+=1
    if m[1]: b+=1
  if 2<len(m[1])<14 and 7<len(m[0])<9 : ac1.append(l)

len(conversation1), a == b, len(ac1), conversations[0]

len(ac1), ac1[:10]

# поскольку массив слишком большой, то уменьшаем его
conversations = conversation1[0:7323] + conversation1[7328:7330]
len(conversations)

conversation2 = conversation1[7320:7329] 
conversation1[7327]

import pandas as pd
df = pd.Series(conversation2)
df

"""### Парсинг 2"""

######################
# Открываем файл с диалогами
######################
print('Количество пар вопрос-ответ : {}'.format(len(conversations)))
print('Пример диалога : {}'.format(conversations[12]))

######################
# Разбираем вопросы-ответы с проставлением тегов ответам
######################
# Собираем вопросы и ответы в списки
questions = list() # здесь будет список вопросов
answers = list() # здесь будет список ответов

# В каждом диалоге берем фразу и добавляем в лист
# Если в ответе не одна фраза - то сцепляем сколько есть
for con in conversations: # для каждой пары вопрос-ответ
  if len(con) > 2 : # если ответ содержит более двух предложений (кол-во реплик, кол-во вариантов ответа)
    questions.append(con[0]) # то вопросительную реплику отправляем в список вопросов
    replies = con[1:] # а ответную составляем из последующих строк
    ans = '' # здесь соберем ответ
    for rep in replies: # каждую реплику в ответной реплике
      ans += ' ' + rep 
    answers.append(ans) #добавим в список ответов
  elif len(con)> 1: # если на 1 вопрос приходится 1 ответ
    questions.append(con[0]) # то вопросительную реплику отправляем в список вопросов
    answers.append(con[1]) # а ответную в список ответов

# Очищаем строки с неопределенным типов ответов
answersCleaned = list()
for i in range(len(answers)):
  if type(answers[i]) == str:
    answersCleaned.append(answers[i]) #если тип - строка, то добавляем в ответы
  else:
    questions.pop(i) # если не строка, то ответ не добавился, и плюс убираем соответствующий вопрос

# Сделаем теги-метки для начала и конца ответов
answers = list()
for i in range(len(answersCleaned)):
  answers.append( '<START> ' + answersCleaned[i] + ' <END>' )

# Выведем обновленные данные на экран
print('Вопрос : {}'.format(questions[20]))
print('Ответ : {}'.format(answers[20]))

# Выведем обновленные данные на экран
print('Вопрос : {}'.format(questions[10]))
print('Ответ : {}'.format(answers[10]))

######################
# Подключаем керасовский токенизатор и собираем словарь индексов
######################
tokenizer = Tokenizer(filters='"!#$%&()*+,-–—./…:;<=>?@[\\]^_`{|}~«»\t\n\xa0\ufeff')
#tokenizer = Tokenizer()
tokenizer.fit_on_texts(questions + answers) # загружаем в токенизатор список вопросов-ответов для сборки словаря частотности
vocabularyItems = list(tokenizer.word_index.items()) # список с cодержимым словаря
vocabularySize = len(vocabularyItems)+1 # размер словаря
print( 'Фрагмент словаря : {}'.format(vocabularyItems[:50]))
print( 'Размер словаря : {}'.format(vocabularySize))

"""## Подготовка выборки"""

######################
# Устанавливаем закодированные входные данные(вопросы)
######################
tokenizedQuestions = tokenizer.texts_to_sequences(questions) # разбиваем текст вопросов на последовательности индексов
maxLenQuestions = max([ len(x) for x in tokenizedQuestions]) # уточняем длину самого большого вопроса
# Делаем последовательности одной длины, заполняя нулями более короткие вопросы
paddedQuestions = pad_sequences(tokenizedQuestions, maxlen=maxLenQuestions, padding='post')

# Предподготавливаем данные для входа в сеть
encoderForInput = np.array(paddedQuestions) # переводим в numpy массив
print('Пример оригинального вопроса на вход : {}'.format(questions[10])) 
print('Пример кодированного вопроса на вход : {}'.format(encoderForInput[10])) 
print('Размеры закодированного массива вопросов на вход : {}'.format(encoderForInput.shape)) 
print('Установленная длина вопросов на вход : {}'.format(maxLenQuestions))

######################
# Устанавливаем раскодированные входные данные(ответы)
######################
tokenizedAnswers = tokenizer.texts_to_sequences(answers) # разбиваем текст ответов на последовательности индексов
maxLenAnswers = max([len(x) for x in tokenizedAnswers]) # уточняем длину самого большого ответа
# Делаем последовательности одной длины, заполняя нулями более короткие ответы
paddedAnswers = pad_sequences(tokenizedAnswers, maxlen=maxLenAnswers, padding='post')

# Предподготавливаем данные для входа в сеть
decoderForInput = np.array(paddedAnswers) # переводим в numpy массив
print('Пример оригинального ответа на вход: {}'.format(answers[10])) 
print('Пример раскодированного ответа на вход : {}'.format(decoderForInput[10][:30])) 
print('Размеры раскодированного массива ответов на вход : {}'.format(decoderForInput.shape)) 
print('Установленная длина ответов на вход : {}'.format(maxLenAnswers))

######################
# Раскодированные выходные данные(ответы)
######################
tokenizedAnswers = tokenizer.texts_to_sequences(answers) # разбиваем текст ответов на последовательности индексов
for i in range(len(tokenizedAnswers)): # для разбитых на последовательности ответов
  tokenizedAnswers[i] = tokenizedAnswers[i][1:] # избавляемся от тега <START>
# Делаем последовательности одной длины, заполняя нулями более короткие ответы
paddedAnswers = pad_sequences(tokenizedAnswers, maxlen=maxLenAnswers , padding='post')

oneHotAnswers = utils.to_categorical(paddedAnswers, vocabularySize) # переводим в one hot vector
decoderForOutput = np.array(oneHotAnswers) # и сохраняем в виде массива numpy

print('Пример раскодированного ответа на вход : {}'.format(decoderForInput[10][:21]))  
print('Пример раскодированного ответа на выход : {}'.format(decoderForOutput[10][4][:21])) 
print('Размеры раскодированного массива ответов на выход : {}'.format(decoderForOutput.shape))
print('Установленная длина вопросов на выход : {}'.format(maxLenAnswers))

type(decoderForInput)

#os.chdir('/content/drive/My Drive/Занятие_13_Генерация текста/data')
#np.save('decoderForInput', decoderForInput)
#np.save('encoderForInput', encoderForInput)
#np.save('decoderForOutput', decoderForOutput)

print(len(decoderForInput), len(decoderForOutput), len(encoderForInput))

"""## Функции для ведения диалога"""

######################
# Создаем рабочую модель для вывода ответов на запросы пользователя
######################
def makeInferenceModels():
  # Определим модель кодера, на входе далее будут закодированные вопросы(encoderForInputs), на выходе состояния state_h, state_c
  encoderModel = Model(encoderInputs, encoderStates) 

  decoderStateInput_h = Input(shape=(200 ,)) # обозначим размерность для входного слоя с состоянием state_h
  decoderStateInput_c = Input(shape=(200 ,)) # обозначим размерность для входного слоя с состоянием state_c

  decoderStatesInputs = [decoderStateInput_h, decoderStateInput_c] # возьмем оба inputs вместе и запишем в decoderStatesInputs

  # Берём ответы, прошедшие через эмбединг, вместе с состояниями и подаём LSTM cлою

  decoderOutputs, state_h, state_c = decoderLSTM(decoderEmbedding, initial_state=decoderStatesInputs)

  decoderStates = [state_h, state_c] # LSTM даст нам новые состояния
  decoderOutputs = decoderDense(decoderOutputs) # и ответы, которые мы пропустим через полносвязный слой с софтмаксом

  # Определим модель декодера, на входе далее будут раскодированные ответы (decoderForInputs) и состояния
  # на выходе предсказываемый ответ и новые состояния
  decoderModel = Model([decoderInputs] + decoderStatesInputs, [decoderOutputs] + decoderStates)

  return encoderModel , decoderModel

######################
# Создадим функцию, которая преобразует вопрос пользователя в последовательность индексов
######################

def strToTokens(sentence: str): # функция принимает строку на вход (предложение с вопросом)
  words = sentence.lower().split() # приводит предложение к нижнему регистру и разбирает на слова
  tokensList = list() # здесь будет последовательность токенов/индексов
  for word in words: # для каждого слова в предложении
    tokensList.append(tokenizer.word_index[word]) # определяем токенизатором индекс и добавляем в список

    # Функция вернёт вопрос в виде последовательности индексов, ограниченной длиной самого длинного вопроса из нашей базы вопросов
  return pad_sequences([tokensList], maxlen=maxLenQuestions , padding='post')

"""## Нейросеть с учебными настройками

### создаем модель
"""

######################
# Первый входной слой, кодер, выходной слой
######################
encoderInputs = Input(shape=(maxLenQuestions, )) # размеры на входе сетки (здесь будет encoderForInput)
# Эти данные проходят через слой Embedding (длина словаря, размерность) 
encoderEmbedding = Embedding(vocabularySize, 200,  mask_zero=True) (encoderInputs)
# Затем выход с Embedding пойдёт в LSTM слой, на выходе у которого будет два вектора состояния - state_h , state_c
# Вектора состояния - state_h , state_c зададутся в LSTM слое декодера в блоке ниже
encoderOutputs, state_h , state_c = LSTM(200, return_state=True)(encoderEmbedding)
encoderStates = [state_h, state_c]

######################
# Второй входной слой, декодер, выходной слой
######################
decoderInputs = Input(shape=(maxLenAnswers, )) # размеры на входе сетки (здесь будет decoderForInput)
# Эти данные проходят через слой Embedding (длина словаря, размерность) 
# mask_zero=True - игнорировать нулевые padding при передаче в LSTM. Предотвратит вывод ответа типа: "У меня все хорошо PAD PAD PAD PAD PAD PAD.."
decoderEmbedding = Embedding(vocabularySize, 200, mask_zero=True) (decoderInputs) 
# Затем выход с Embedding пойдёт в LSTM слой, которому передаются вектора состояния - state_h , state_c

decoderLSTM = LSTM(200, return_state=True, return_sequences=True)


decoderOutputs , _ , _ = decoderLSTM (decoderEmbedding, initial_state=encoderStates)
# И от LSTM'а сигнал decoderOutputs пропускаем через полносвязный слой с софтмаксом на выходе
decoderDense = Dense(vocabularySize, activation='softmax') 
output = decoderDense (decoderOutputs)

######################
# Собираем тренировочную модель нейросети
######################
model = Model([encoderInputs, decoderInputs], output)
model.compile(optimizer=Adam(), loss='categorical_crossentropy')

print(model.summary()) # выведем на экран информацию о построенной модели нейросети
plot_model(model, dpi=50, show_shapes=True, show_layer_names=True)

"""### Обучение"""

#model.load_weights('/content/drive/My Drive/Занятие_12_Сегментация/data_стройка/modelPro_v2.h5')

#Коллбэки
early_stopping = EarlyStopping(monitor = 'loss', patience = 12, verbose = 0, restore_best_weights = True)
#reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.6, patience=6, min_lr=1e-06, verbose = 1)
Checkpoint = ModelCheckpoint('modelPro_v2.h5', monitor='loss', verbose=1, save_best_only=True, save_weights_only=True)

history = model.fit([encoderForInput , decoderForInput], decoderForOutput, 
                    batch_size=100, epochs=200, verbose=1,
                    callbacks = [early_stopping, Checkpoint])

#Выводим график точности распознавания на обучающей и проверочной выборках
clear_output()
plt.plot(history.history['loss'], label='loss')
plt.legend()
plt.grid()
plt.show()

"""### Диалог"""

######################
# Устанавливаем окончательные настройки и запускаем модель
######################

encModel, decModel = makeInferenceModels() # запускаем функцию для построения модели кодера и декодера

for _ in range(6): # задаем количество вопросов, и на каждой итерации в этом диапазоне:
  # Получаем значения состояний, которые определит кодер в соответствии с заданным вопросом
  statesValues = encModel.predict(strToTokens(input( 'Задайте вопрос : ' )))
  # Создаём пустой массив размером (1, 1)
  emptyTargetSeq = np.zeros((1, 1))    
  emptyTargetSeq[0, 0] = tokenizer.word_index['start'] # положим в пустую последовательность начальное слово 'start' в виде индекса

  stopCondition = False # зададим условие, при срабатывании которого, прекратится генерация очередного слова
  decodedTranslation = '' # здесь будет собираться генерируемый ответ
  while not stopCondition : # пока не сработало стоп-условие
    # В модель декодера подадим пустую последовательность со словом 'start' и состояния предсказанные кодером по заданному вопросу.
    # декодер заменит слово 'start' предсказанным сгенерированным словом и обновит состояния
    decOutputs , h , c = decModel.predict([emptyTargetSeq] + statesValues)
    
    #argmax пробежит по вектору decOutputs'а[0,0,15104], найдет макс.значение, и вернёт нам номер индекса под которым оно лежит в массиве
    sampledWordIndex = np.argmax(decOutputs, axis=-1) # argmax возьмем от оси, в которой 15104 элементов. Получили индекс предсказанного слова.
    sampledWord = None # создаем переменную, в которую положим слово, преобразованное на естественный язык
    for word , index in tokenizer.word_index.items():
      if sampledWordIndex == index: # если индекс выбранного слова соответствует какому-то индексу из словаря
        decodedTranslation += ' {}'.format(word) # слово, идущее под этим индексом в словаре, добавляется в итоговый ответ 
        sampledWord = word # выбранное слово фиксируем в переменную sampledWord
    
    # Если выбранным словом оказывается 'end' либо если сгенерированный ответ превышает заданную максимальную длину ответа
    if sampledWord == 'end' or len(decodedTranslation.split()) > maxLenAnswers:
      stopCondition = True # то срабатывает стоп-условие и прекращаем генерацию

    emptyTargetSeq = np.zeros((1, 1)) # создаем пустой массив
    emptyTargetSeq[0, 0] = sampledWordIndex # заносим туда индекс выбранного слова
    statesValues = [h, c] # и состояния, обновленные декодером
    # и продолжаем цикл с обновленными параметрами
  
  print(decodedTranslation[:-3]) # выводим ответ сгенерированный декодером



"""## Эксперименты с оптимизаторами на 100 эпохах"""

os.chdir('/content/drive/My Drive/Занятие_13_Генерация текста/data')
#model.load_weights('/content/drive/My Drive/Занятие_12_Сегментация/data_стройка/modelPro_v2_%r.h5'%(opt))

optim = ['RMSprop']
for opt in optim:

  ######################
  # Первый входной слой, кодер, выходной слой
  ######################
  encoderInputs = Input(shape=(11, )) # размеры на входе сетки (здесь будет encoderForInput)
  # Эти данные проходят через слой Embedding (длина словаря, размерность) 
  encoderEmbedding = Embedding(vocabularySize, 200,  mask_zero=True) (encoderInputs)
  # Затем выход с Embedding пойдёт в LSTM слой, на выходе у которого будет два вектора состояния - state_h , state_c
  # Вектора состояния - state_h , state_c зададутся в LSTM слое декодера в блоке ниже
  encoderOutputs, state_h , state_c = LSTM(200, return_state=True)(encoderEmbedding)
  encoderStates = [state_h, state_c]

  ######################
  # Второй входной слой, декодер, выходной слой
  ######################
  decoderInputs = Input(shape=(13, )) # размеры на входе сетки (здесь будет decoderForInput)
  # Эти данные проходят через слой Embedding (длина словаря, размерность) 
  # mask_zero=True - игнорировать нулевые padding при передаче в LSTM. Предотвратит вывод ответа типа: "У меня все хорошо PAD PAD PAD PAD PAD PAD.."
  decoderEmbedding = Embedding(vocabularySize, 200, mask_zero=True) (decoderInputs) 
  # Затем выход с Embedding пойдёт в LSTM слой, которому передаются вектора состояния - state_h , state_c

  decoderLSTM = LSTM(200, return_state=True, return_sequences=True)


  decoderOutputs , _ , _ = decoderLSTM (decoderEmbedding, initial_state=encoderStates)
  # И от LSTM'а сигнал decoderOutputs пропускаем через полносвязный слой с софтмаксом на выходе
  decoderDense = Dense(vocabularySize, activation='softmax') 
  output = decoderDense (decoderOutputs)

  model = Model([encoderInputs, decoderInputs], output)
  model.compile(optimizer = opt, loss='categorical_crossentropy')
  print('-'*90)
  print('100 эпох обучения. Optimizer = ', opt)
  #Коллбэки
  early_stopping = EarlyStopping(monitor = 'loss', patience = 12, verbose = 0, restore_best_weights = True)
  Checkpoint = ModelCheckpoint('modelPro_v2_%r.h5'%(opt), monitor='loss', verbose=0, save_best_only=True, save_weights_only=True)

  history = model.fit([encoderForInput , decoderForInput], decoderForOutput, 
                      batch_size=100, epochs=100, verbose=0,
                      callbacks = [early_stopping, Checkpoint])

  #Выводим график точности распознавания на обучающей и проверочной выборках
  #clear_output()
  plt.plot(history.history['loss'], label='loss')
  plt.legend()
  plt.grid()
  plt.show()

  ######################
  # Устанавливаем окончательные настройки и запускаем модель
  ######################

  encModel, decModel = makeInferenceModels() # запускаем функцию для построения модели кодера и декодера

  for _ in range(6): # задаем количество вопросов, и на каждой итерации в этом диапазоне:
    # Получаем значения состояний, которые определит кодер в соответствии с заданным вопросом
    statesValues = encModel.predict(strToTokens(input( 'Задайте вопрос : ' )))
    # Создаём пустой массив размером (1, 1)
    emptyTargetSeq = np.zeros((1, 1))    
    emptyTargetSeq[0, 0] = tokenizer.word_index['start'] # положим в пустую последовательность начальное слово 'start' в виде индекса

    stopCondition = False # зададим условие, при срабатывании которого, прекратится генерация очередного слова
    decodedTranslation = '' # здесь будет собираться генерируемый ответ
    while not stopCondition : # пока не сработало стоп-условие
      # В модель декодера подадим пустую последовательность со словом 'start' и состояния предсказанные кодером по заданному вопросу.
      # декодер заменит слово 'start' предсказанным сгенерированным словом и обновит состояния
      decOutputs , h , c = decModel.predict([emptyTargetSeq] + statesValues)
      
      #argmax пробежит по вектору decOutputs'а[0,0,15104], найдет макс.значение, и вернёт нам номер индекса под которым оно лежит в массиве
      sampledWordIndex = np.argmax(decOutputs, axis=-1) # argmax возьмем от оси, в которой 15104 элементов. Получили индекс предсказанного слова.
      sampledWord = None # создаем переменную, в которую положим слово, преобразованное на естественный язык
      for word , index in tokenizer.word_index.items():
        if sampledWordIndex == index: # если индекс выбранного слова соответствует какому-то индексу из словаря
          decodedTranslation += ' {}'.format(word) # слово, идущее под этим индексом в словаре, добавляется в итоговый ответ 
          sampledWord = word # выбранное слово фиксируем в переменную sampledWord
      
      # Если выбранным словом оказывается 'end' либо если сгенерированный ответ превышает заданную максимальную длину ответа
      if sampledWord == 'end' or len(decodedTranslation.split()) > maxLenAnswers:
        stopCondition = True # то срабатывает стоп-условие и прекращаем генерацию

      emptyTargetSeq = np.zeros((1, 1)) # создаем пустой массив
      emptyTargetSeq[0, 0] = sampledWordIndex # заносим туда индекс выбранного слова
      statesValues = [h, c] # и состояния, обновленные декодером
      # и продолжаем цикл с обновленными параметрами
    
    print(decodedTranslation[:-3]) # выводим ответ сгенерированный декодером

os.chdir('/content/drive/My Drive/Занятие_13_Генерация текста/data')
#model.load_weights('/content/drive/My Drive/Занятие_12_Сегментация/data_стройка/modelPro_v2_%r.h5'%(opt))

optim = ['Adam']
for opt in optim:

  ######################
  # Первый входной слой, кодер, выходной слой
  ######################
  encoderInputs = Input(shape=(11, )) # размеры на входе сетки (здесь будет encoderForInput)
  # Эти данные проходят через слой Embedding (длина словаря, размерность) 
  encoderEmbedding = Embedding(vocabularySize, 200,  mask_zero=True) (encoderInputs)
  # Затем выход с Embedding пойдёт в LSTM слой, на выходе у которого будет два вектора состояния - state_h , state_c
  # Вектора состояния - state_h , state_c зададутся в LSTM слое декодера в блоке ниже
  encoderOutputs, state_h , state_c = LSTM(200, return_state=True)(encoderEmbedding)
  encoderStates = [state_h, state_c]

  ######################
  # Второй входной слой, декодер, выходной слой
  ######################
  decoderInputs = Input(shape=(13, )) # размеры на входе сетки (здесь будет decoderForInput)
  # Эти данные проходят через слой Embedding (длина словаря, размерность) 
  # mask_zero=True - игнорировать нулевые padding при передаче в LSTM. Предотвратит вывод ответа типа: "У меня все хорошо PAD PAD PAD PAD PAD PAD.."
  decoderEmbedding = Embedding(vocabularySize, 200, mask_zero=True) (decoderInputs) 
  # Затем выход с Embedding пойдёт в LSTM слой, которому передаются вектора состояния - state_h , state_c

  decoderLSTM = LSTM(200, return_state=True, return_sequences=True)


  decoderOutputs , _ , _ = decoderLSTM (decoderEmbedding, initial_state=encoderStates)
  # И от LSTM'а сигнал decoderOutputs пропускаем через полносвязный слой с софтмаксом на выходе
  decoderDense = Dense(vocabularySize, activation='softmax') 
  output = decoderDense (decoderOutputs)

  model = Model([encoderInputs, decoderInputs], output)
  model.compile(optimizer = opt, loss='categorical_crossentropy')
  print('-'*90)
  print('100 эпох обучения. Optimizer = ', opt)
  #Коллбэки
  early_stopping = EarlyStopping(monitor = 'loss', patience = 12, verbose = 0, restore_best_weights = True)
  Checkpoint = ModelCheckpoint('modelPro_v2_%r.h5'%(opt), monitor='loss', verbose=0, save_best_only=True, save_weights_only=True)

  history = model.fit([encoderForInput , decoderForInput], decoderForOutput, 
                      batch_size=100, epochs=100, verbose=0,
                      callbacks = [early_stopping, Checkpoint])

  #Выводим график точности распознавания на обучающей и проверочной выборках
  #clear_output()
  plt.plot(history.history['loss'], label='loss')
  plt.legend()
  plt.grid()
  plt.show()

  ######################
  # Устанавливаем окончательные настройки и запускаем модель
  ######################

  encModel, decModel = makeInferenceModels() # запускаем функцию для построения модели кодера и декодера

  for _ in range(6): # задаем количество вопросов, и на каждой итерации в этом диапазоне:
    # Получаем значения состояний, которые определит кодер в соответствии с заданным вопросом
    statesValues = encModel.predict(strToTokens(input( 'Задайте вопрос : ' )))
    # Создаём пустой массив размером (1, 1)
    emptyTargetSeq = np.zeros((1, 1))    
    emptyTargetSeq[0, 0] = tokenizer.word_index['start'] # положим в пустую последовательность начальное слово 'start' в виде индекса

    stopCondition = False # зададим условие, при срабатывании которого, прекратится генерация очередного слова
    decodedTranslation = '' # здесь будет собираться генерируемый ответ
    while not stopCondition : # пока не сработало стоп-условие
      # В модель декодера подадим пустую последовательность со словом 'start' и состояния предсказанные кодером по заданному вопросу.
      # декодер заменит слово 'start' предсказанным сгенерированным словом и обновит состояния
      decOutputs , h , c = decModel.predict([emptyTargetSeq] + statesValues)
      
      #argmax пробежит по вектору decOutputs'а[0,0,15104], найдет макс.значение, и вернёт нам номер индекса под которым оно лежит в массиве
      sampledWordIndex = np.argmax(decOutputs, axis=-1) # argmax возьмем от оси, в которой 15104 элементов. Получили индекс предсказанного слова.
      sampledWord = None # создаем переменную, в которую положим слово, преобразованное на естественный язык
      for word , index in tokenizer.word_index.items():
        if sampledWordIndex == index: # если индекс выбранного слова соответствует какому-то индексу из словаря
          decodedTranslation += ' {}'.format(word) # слово, идущее под этим индексом в словаре, добавляется в итоговый ответ 
          sampledWord = word # выбранное слово фиксируем в переменную sampledWord
      
      # Если выбранным словом оказывается 'end' либо если сгенерированный ответ превышает заданную максимальную длину ответа
      if sampledWord == 'end' or len(decodedTranslation.split()) > maxLenAnswers:
        stopCondition = True # то срабатывает стоп-условие и прекращаем генерацию

      emptyTargetSeq = np.zeros((1, 1)) # создаем пустой массив
      emptyTargetSeq[0, 0] = sampledWordIndex # заносим туда индекс выбранного слова
      statesValues = [h, c] # и состояния, обновленные декодером
      # и продолжаем цикл с обновленными параметрами
    
    print(decodedTranslation[:-3]) # выводим ответ сгенерированный декодером

os.chdir('/content/drive/My Drive/Занятие_13_Генерация текста/data')
#model.load_weights('/content/drive/My Drive/Занятие_12_Сегментация/data_стройка/modelPro_v2_%r.h5'%(opt))

optim = ['Nadam']
for opt in optim:

  ######################
  # Первый входной слой, кодер, выходной слой
  ######################
  encoderInputs = Input(shape=(11, )) # размеры на входе сетки (здесь будет encoderForInput)
  # Эти данные проходят через слой Embedding (длина словаря, размерность) 
  encoderEmbedding = Embedding(vocabularySize, 200,  mask_zero=True) (encoderInputs)
  # Затем выход с Embedding пойдёт в LSTM слой, на выходе у которого будет два вектора состояния - state_h , state_c
  # Вектора состояния - state_h , state_c зададутся в LSTM слое декодера в блоке ниже
  encoderOutputs, state_h , state_c = LSTM(200, return_state=True)(encoderEmbedding)
  encoderStates = [state_h, state_c]

  ######################
  # Второй входной слой, декодер, выходной слой
  ######################
  decoderInputs = Input(shape=(13, )) # размеры на входе сетки (здесь будет decoderForInput)
  # Эти данные проходят через слой Embedding (длина словаря, размерность) 
  # mask_zero=True - игнорировать нулевые padding при передаче в LSTM. Предотвратит вывод ответа типа: "У меня все хорошо PAD PAD PAD PAD PAD PAD.."
  decoderEmbedding = Embedding(vocabularySize, 200, mask_zero=True) (decoderInputs) 
  # Затем выход с Embedding пойдёт в LSTM слой, которому передаются вектора состояния - state_h , state_c

  decoderLSTM = LSTM(200, return_state=True, return_sequences=True)


  decoderOutputs , _ , _ = decoderLSTM (decoderEmbedding, initial_state=encoderStates)
  # И от LSTM'а сигнал decoderOutputs пропускаем через полносвязный слой с софтмаксом на выходе
  decoderDense = Dense(vocabularySize, activation='softmax') 
  output = decoderDense (decoderOutputs)

  model = Model([encoderInputs, decoderInputs], output)
  model.compile(optimizer = opt, loss='categorical_crossentropy')
  print('-'*90)
  print('100 эпох обучения. Optimizer = ', opt)
  #Коллбэки
  early_stopping = EarlyStopping(monitor = 'loss', patience = 12, verbose = 0, restore_best_weights = True)
  Checkpoint = ModelCheckpoint('modelPro_v2_%r.h5'%(opt), monitor='loss', verbose=0, save_best_only=True, save_weights_only=True)

  history = model.fit([encoderForInput , decoderForInput], decoderForOutput, 
                      batch_size=100, epochs=100, verbose=0,
                      callbacks = [early_stopping, Checkpoint])

  #Выводим график точности распознавания на обучающей и проверочной выборках
  #clear_output()
  plt.plot(history.history['loss'], label='loss')
  plt.legend()
  plt.grid()
  plt.show()

  ######################
  # Устанавливаем окончательные настройки и запускаем модель
  ######################

  encModel, decModel = makeInferenceModels() # запускаем функцию для построения модели кодера и декодера

  for _ in range(6): # задаем количество вопросов, и на каждой итерации в этом диапазоне:
    # Получаем значения состояний, которые определит кодер в соответствии с заданным вопросом
    statesValues = encModel.predict(strToTokens(input( 'Задайте вопрос : ' )))
    # Создаём пустой массив размером (1, 1)
    emptyTargetSeq = np.zeros((1, 1))    
    emptyTargetSeq[0, 0] = tokenizer.word_index['start'] # положим в пустую последовательность начальное слово 'start' в виде индекса

    stopCondition = False # зададим условие, при срабатывании которого, прекратится генерация очередного слова
    decodedTranslation = '' # здесь будет собираться генерируемый ответ
    while not stopCondition : # пока не сработало стоп-условие
      # В модель декодера подадим пустую последовательность со словом 'start' и состояния предсказанные кодером по заданному вопросу.
      # декодер заменит слово 'start' предсказанным сгенерированным словом и обновит состояния
      decOutputs , h , c = decModel.predict([emptyTargetSeq] + statesValues)
      
      #argmax пробежит по вектору decOutputs'а[0,0,15104], найдет макс.значение, и вернёт нам номер индекса под которым оно лежит в массиве
      sampledWordIndex = np.argmax(decOutputs, axis=-1) # argmax возьмем от оси, в которой 15104 элементов. Получили индекс предсказанного слова.
      sampledWord = None # создаем переменную, в которую положим слово, преобразованное на естественный язык
      for word , index in tokenizer.word_index.items():
        if sampledWordIndex == index: # если индекс выбранного слова соответствует какому-то индексу из словаря
          decodedTranslation += ' {}'.format(word) # слово, идущее под этим индексом в словаре, добавляется в итоговый ответ 
          sampledWord = word # выбранное слово фиксируем в переменную sampledWord
      
      # Если выбранным словом оказывается 'end' либо если сгенерированный ответ превышает заданную максимальную длину ответа
      if sampledWord == 'end' or len(decodedTranslation.split()) > maxLenAnswers:
        stopCondition = True # то срабатывает стоп-условие и прекращаем генерацию

      emptyTargetSeq = np.zeros((1, 1)) # создаем пустой массив
      emptyTargetSeq[0, 0] = sampledWordIndex # заносим туда индекс выбранного слова
      statesValues = [h, c] # и состояния, обновленные декодером
      # и продолжаем цикл с обновленными параметрами
    
    print(decodedTranslation[:-3]) # выводим ответ сгенерированный декодером

os.chdir('/content/drive/My Drive/Занятие_13_Генерация текста/data')
#model.load_weights('/content/drive/My Drive/Занятие_12_Сегментация/data_стройка/modelPro_v2_%r.h5'%(opt))

optim = ['Adamax']
for opt in optim:

  ######################
  # Первый входной слой, кодер, выходной слой
  ######################
  encoderInputs = Input(shape=(11, )) # размеры на входе сетки (здесь будет encoderForInput)
  # Эти данные проходят через слой Embedding (длина словаря, размерность) 
  encoderEmbedding = Embedding(vocabularySize, 200,  mask_zero=True) (encoderInputs)
  # Затем выход с Embedding пойдёт в LSTM слой, на выходе у которого будет два вектора состояния - state_h , state_c
  # Вектора состояния - state_h , state_c зададутся в LSTM слое декодера в блоке ниже
  encoderOutputs, state_h , state_c = LSTM(200, return_state=True)(encoderEmbedding)
  encoderStates = [state_h, state_c]

  ######################
  # Второй входной слой, декодер, выходной слой
  ######################
  decoderInputs = Input(shape=(13, )) # размеры на входе сетки (здесь будет decoderForInput)
  # Эти данные проходят через слой Embedding (длина словаря, размерность) 
  # mask_zero=True - игнорировать нулевые padding при передаче в LSTM. Предотвратит вывод ответа типа: "У меня все хорошо PAD PAD PAD PAD PAD PAD.."
  decoderEmbedding = Embedding(vocabularySize, 200, mask_zero=True) (decoderInputs) 
  # Затем выход с Embedding пойдёт в LSTM слой, которому передаются вектора состояния - state_h , state_c

  decoderLSTM = LSTM(200, return_state=True, return_sequences=True)


  decoderOutputs , _ , _ = decoderLSTM (decoderEmbedding, initial_state=encoderStates)
  # И от LSTM'а сигнал decoderOutputs пропускаем через полносвязный слой с софтмаксом на выходе
  decoderDense = Dense(vocabularySize, activation='softmax') 
  output = decoderDense (decoderOutputs)

  model = Model([encoderInputs, decoderInputs], output)
  model.compile(optimizer = opt, loss='categorical_crossentropy')
  print('-'*90)
  print('200 эпох обучения. Optimizer = ', opt)
  #Коллбэки
  early_stopping = EarlyStopping(monitor = 'loss', patience = 12, verbose = 0, restore_best_weights = True)
  Checkpoint = ModelCheckpoint('modelPro_v2_%r.h5'%(opt), monitor='loss', verbose=0, save_best_only=True, save_weights_only=True)

  history = model.fit([encoderForInput , decoderForInput], decoderForOutput, 
                      batch_size=100, epochs=200, verbose=0,
                      callbacks = [early_stopping, Checkpoint])

  #Выводим график точности распознавания на обучающей и проверочной выборках
  #clear_output()
  plt.plot(history.history['loss'], label='loss')
  plt.legend()
  plt.grid()
  plt.show()

  ######################
  # Устанавливаем окончательные настройки и запускаем модель
  ######################

  encModel, decModel = makeInferenceModels() # запускаем функцию для построения модели кодера и декодера

  for _ in range(6): # задаем количество вопросов, и на каждой итерации в этом диапазоне:
    # Получаем значения состояний, которые определит кодер в соответствии с заданным вопросом
    statesValues = encModel.predict(strToTokens(input( 'Задайте вопрос : ' )))
    # Создаём пустой массив размером (1, 1)
    emptyTargetSeq = np.zeros((1, 1))    
    emptyTargetSeq[0, 0] = tokenizer.word_index['start'] # положим в пустую последовательность начальное слово 'start' в виде индекса

    stopCondition = False # зададим условие, при срабатывании которого, прекратится генерация очередного слова
    decodedTranslation = '' # здесь будет собираться генерируемый ответ
    while not stopCondition : # пока не сработало стоп-условие
      # В модель декодера подадим пустую последовательность со словом 'start' и состояния предсказанные кодером по заданному вопросу.
      # декодер заменит слово 'start' предсказанным сгенерированным словом и обновит состояния
      decOutputs , h , c = decModel.predict([emptyTargetSeq] + statesValues)
      
      #argmax пробежит по вектору decOutputs'а[0,0,15104], найдет макс.значение, и вернёт нам номер индекса под которым оно лежит в массиве
      sampledWordIndex = np.argmax(decOutputs, axis=-1) # argmax возьмем от оси, в которой 15104 элементов. Получили индекс предсказанного слова.
      sampledWord = None # создаем переменную, в которую положим слово, преобразованное на естественный язык
      for word , index in tokenizer.word_index.items():
        if sampledWordIndex == index: # если индекс выбранного слова соответствует какому-то индексу из словаря
          decodedTranslation += ' {}'.format(word) # слово, идущее под этим индексом в словаре, добавляется в итоговый ответ 
          sampledWord = word # выбранное слово фиксируем в переменную sampledWord
      
      # Если выбранным словом оказывается 'end' либо если сгенерированный ответ превышает заданную максимальную длину ответа
      if sampledWord == 'end' or len(decodedTranslation.split()) > maxLenAnswers:
        stopCondition = True # то срабатывает стоп-условие и прекращаем генерацию

      emptyTargetSeq = np.zeros((1, 1)) # создаем пустой массив
      emptyTargetSeq[0, 0] = sampledWordIndex # заносим туда индекс выбранного слова
      statesValues = [h, c] # и состояния, обновленные декодером
      # и продолжаем цикл с обновленными параметрами
    
    print(decodedTranslation[:-3]) # выводим ответ сгенерированный декодером

"""# Выводы:
1. Сначала я обучил сеть с урока на 200 эпохах, чтобы посмотреть на достигаемый ей loss:
*результат ужасный!!! сетка переучилась*
2. Потом проверил влияние различных оптимайзеров на качество обучения но ограничился 100 эпохами чтобы не переучивать
3. Я достиг почти 0.01 на оптимайзерах Adam и Nadam. Судя по ответам сети - нет смысла еще улучшать обучение, так как естьпроблемы в обучающей выборке. Лучше не будет
"""